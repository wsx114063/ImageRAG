{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "80495cc0",
      "metadata": {
        "id": "80495cc0"
      },
      "source": [
        "# CUB-200-2011 Bird Retrieval with Description Embedding\n",
        "\n",
        "é€™å€‹ notebook çµåˆ **åœ–ç‰‡ embedding** å’Œ **æ–‡å­—æè¿° embedding** ä¾†å»ºç«‹æ›´ç²¾æº–çš„é³¥é¡åœ–ç‰‡æª¢ç´¢ç³»çµ±ã€‚\n",
        "\n",
        "## å¯ç”¨è³‡æ–™ä¾†æºï¼š\n",
        "| è³‡æ–™ä¾†æº | é¡å‹ | ç”¨é€” |\n",
        "|---------|------|------|\n",
        "| `cvpr2016_cub/text_c10/` | æ¯å¼µåœ– 10 æ¢è‡ªç„¶èªè¨€æè¿° | â­ ä¸»è¦æ–‡å­— embedding |\n",
        "| `attributes/attributes.txt` | 312 ç¨®è¦–è¦ºå±¬æ€§ | è¼”åŠ©å±¬æ€§æè¿° |\n",
        "| `classes.txt` | 200 ç¨®é³¥é¡åç¨± | é¡åˆ¥è³‡è¨Š |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "eaac875d",
      "metadata": {
        "id": "eaac875d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "275fb900-af2d-4933-a06b-e9d4ab809c68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-8mt4kqov\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-8mt4kqov\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.13.0-cp39-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Collecting ftfy (from clip==1.0)\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (4.67.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (0.24.0+cu126)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy->clip==1.0) (0.2.14)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.5.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->clip==1.0) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->clip==1.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->clip==1.0) (3.0.3)\n",
            "Downloading faiss_cpu-1.13.0-cp39-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (23.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.6/23.6 MB\u001b[0m \u001b[31m119.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369490 sha256=ca804e615c9642c2dfcd4aab236bd29f704ba3c615a335908c7855cd9d331f35\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-z1va3drc/wheels/35/3e/df/3d24cbfb3b6a06f17a2bfd7d1138900d4365d9028aa8f6e92f\n",
            "Successfully built clip\n",
            "Installing collected packages: ftfy, faiss-cpu, clip\n",
            "Successfully installed clip-1.0 faiss-cpu-1.13.0 ftfy-6.3.1\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.12/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.12/dist-packages (from kaggle) (6.3.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2025.11.12)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.4.4)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.11)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.12/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.32.4)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from kaggle) (0.5.1)\n"
          ]
        }
      ],
      "source": [
        "# å®‰è£å¿…è¦å¥—ä»¶\n",
        "!pip install faiss-cpu git+https://github.com/openai/CLIP.git\n",
        "!pip install kaggle tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "186b53a1",
      "metadata": {
        "id": "186b53a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "6b7227eb-8c08-4fa2-e219-9b5b8865b513"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-343081ea-595f-4d2b-bd00-92d1a7f50f00\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-343081ea-595f-4d2b-bd00-92d1a7f50f00\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "Dataset URL: https://www.kaggle.com/datasets/wenewone/cub2002011\n",
            "License(s): CC0-1.0\n",
            "Downloading cub2002011.zip to /content\n",
            " 93% 1.39G/1.49G [00:01<00:00, 1.09GB/s]\n",
            "100% 1.49G/1.49G [00:01<00:00, 1.30GB/s]\n"
          ]
        }
      ],
      "source": [
        "# (Colab ç”¨) ä¸Šå‚³ kaggle.json ä¸¦ä¸‹è¼‰è³‡æ–™é›†\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "# ä¸Šå‚³ kaggle.json\n",
        "uploaded = files.upload()\n",
        "\n",
        "# å»ºç«‹ kaggle è³‡æ–™å¤¾\n",
        "os.makedirs(\"/root/.kaggle\", exist_ok=True)\n",
        "os.rename(\"kaggle.json\", \"/root/.kaggle/kaggle.json\")\n",
        "os.chmod(\"/root/.kaggle/kaggle.json\", 600)\n",
        "\n",
        "# ä¸‹è¼‰ CUB 200 2011\n",
        "!kaggle datasets download -d wenewone/cub2002011\n",
        "\n",
        "# è§£å£“\n",
        "!unzip -q cub2002011.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bEftnxaxzNU3",
      "metadata": {
        "id": "bEftnxaxzNU3"
      },
      "outputs": [],
      "source": [
        "!pip install open_clip_torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85661fdf",
      "metadata": {
        "id": "85661fdf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import open_clip\n",
        "from PIL import Image\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model, _, preprocess = open_clip.create_model_and_transforms(\n",
        "    'ViT-bigG-14',\n",
        "    pretrained='laion2b_s39b_b160k'\n",
        ")\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "tokenizer = open_clip.get_tokenizer('ViT-bigG-14')\n",
        "\n",
        "\n",
        "print(f\"ä½¿ç”¨è£ç½®: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a635be4",
      "metadata": {
        "id": "0a635be4"
      },
      "source": [
        "## 1. è¼‰å…¥è³‡æ–™é›†è³‡è¨Š\n",
        "- é¡åˆ¥åç¨± (classes.txt)\n",
        "- å±¬æ€§å®šç¾© (attributes.txt)\n",
        "- åœ–ç‰‡æè¿° (text_c10/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7548b4bc",
      "metadata": {
        "id": "7548b4bc"
      },
      "outputs": [],
      "source": [
        "# è·¯å¾‘è¨­å®š (Colab ç’°å¢ƒ)\n",
        "BASE_PATH = \"/content\"  # æœ¬åœ°è«‹æ”¹æˆä½ çš„è·¯å¾‘\n",
        "IMAGE_FOLDER = os.path.join(BASE_PATH, \"CUB_200_2011/images\")\n",
        "TEXT_FOLDER = os.path.join(BASE_PATH, \"cvpr2016_cub/text_c10\")\n",
        "CLASSES_FILE = os.path.join(BASE_PATH, \"CUB_200_2011/classes.txt\")\n",
        "ATTRIBUTES_FILE = os.path.join(BASE_PATH, \"CUB_200_2011/attributes/attributes.txt\")\n",
        "\n",
        "print(f\"Image folder: {IMAGE_FOLDER}\")\n",
        "print(f\"Text folder: {TEXT_FOLDER}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f063546d",
      "metadata": {
        "id": "f063546d"
      },
      "outputs": [],
      "source": [
        "# è¼‰å…¥é¡åˆ¥åç¨±\n",
        "def load_classes(path):\n",
        "    \"\"\"è®€å– 200 ç¨®é³¥é¡åç¨±\"\"\"\n",
        "    classes = {}\n",
        "    with open(path, \"r\") as f:\n",
        "        for line in f:\n",
        "            idx, name = line.strip().split(\" \", 1)\n",
        "            # æ ¼å¼: \"001.Black_footed_Albatross\" -> \"Black footed Albatross\"\n",
        "            class_name = name.split(\".\")[-1].replace(\"_\", \" \")\n",
        "            classes[int(idx)] = class_name\n",
        "    return classes\n",
        "\n",
        "# è¼‰å…¥å±¬æ€§å®šç¾©\n",
        "def load_attributes(path):\n",
        "    \"\"\"è®€å– 312 ç¨®è¦–è¦ºå±¬æ€§\"\"\"\n",
        "    attributes = {}\n",
        "    with open(path, \"r\") as f:\n",
        "        for line in f:\n",
        "            idx, name = line.strip().split(\" \", 1)\n",
        "            # æ ¼å¼: \"has_bill_shape::curved\" -> \"curved bill shape\"\n",
        "            parts = name.split(\"::\")\n",
        "            if len(parts) == 2:\n",
        "                attr_type = parts[0].replace(\"has_\", \"\").replace(\"_\", \" \")\n",
        "                attr_value = parts[1].replace(\"_\", \" \")\n",
        "                attributes[int(idx)] = f\"{attr_value} {attr_type}\"\n",
        "            else:\n",
        "                attributes[int(idx)] = name.replace(\"_\", \" \")\n",
        "    return attributes\n",
        "\n",
        "# è¼‰å…¥\n",
        "classes = load_classes(CLASSES_FILE)\n",
        "attributes = load_attributes(ATTRIBUTES_FILE)\n",
        "\n",
        "print(f\"é¡åˆ¥æ•¸é‡: {len(classes)}\")\n",
        "print(f\"å±¬æ€§æ•¸é‡: {len(attributes)}\")\n",
        "print(f\"\\nç¯„ä¾‹é¡åˆ¥: {list(classes.items())[:5]}\")\n",
        "print(f\"\\nç¯„ä¾‹å±¬æ€§: {list(attributes.items())[:10]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ae0f2f8",
      "metadata": {
        "id": "4ae0f2f8"
      },
      "outputs": [],
      "source": [
        "# è¼‰å…¥æ¯å¼µåœ–ç‰‡çš„æ–‡å­—æè¿° (text_c10)\n",
        "def load_descriptions(text_folder):\n",
        "    \"\"\"å¾ text_c10 è®€å–æ¯å¼µåœ–ç‰‡çš„ 10 æ¢æ–‡å­—æè¿°\"\"\"\n",
        "    descriptions = {}\n",
        "\n",
        "    for class_dir in os.listdir(text_folder):\n",
        "        class_path = os.path.join(text_folder, class_dir)\n",
        "        if not os.path.isdir(class_path):\n",
        "            continue\n",
        "\n",
        "        for f in os.listdir(class_path):\n",
        "            if f.endswith(\".txt\"):\n",
        "                txt_path = os.path.join(class_path, f)\n",
        "                # åœ–ç‰‡å: Black_Footed_Albatross_0001_796111.txt -> Black_Footed_Albatross_0001_796111.jpg\n",
        "                img_name = f.replace(\".txt\", \".jpg\")\n",
        "                img_key = os.path.join(class_dir, img_name)\n",
        "\n",
        "                with open(txt_path, \"r\", encoding=\"utf-8\") as file:\n",
        "                    descs = [line.strip() for line in file.readlines() if line.strip()]\n",
        "                    descriptions[img_key] = descs\n",
        "\n",
        "    return descriptions\n",
        "\n",
        "descriptions = load_descriptions(TEXT_FOLDER)\n",
        "print(f\"è¼‰å…¥ {len(descriptions)} å¼µåœ–ç‰‡çš„æè¿°\")\n",
        "\n",
        "# é¡¯ç¤ºç¯„ä¾‹\n",
        "sample_key = list(descriptions.keys())[0]\n",
        "print(f\"\\n=== ç¯„ä¾‹åœ–ç‰‡: {sample_key} ===\")\n",
        "print(f\"æè¿°æ•¸é‡: {len(descriptions[sample_key])}\")\n",
        "for i, desc in enumerate(descriptions[sample_key][:5]):\n",
        "    print(f\"  {i+1}. {desc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb9b24fa",
      "metadata": {
        "id": "cb9b24fa"
      },
      "source": [
        "## 2. ç”¢ç”Ÿ Image + Text Combined Embedding\n",
        "\n",
        "çµåˆç­–ç•¥ï¼š\n",
        "- **Image Embedding**: CLIP encode_image\n",
        "- **Text Embedding**: å° 10 æ¢æè¿°å–å¹³å‡ embedding\n",
        "- **Combined Embedding**: Î± Ã— Image + (1-Î±) Ã— Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa370c78",
      "metadata": {
        "id": "aa370c78"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "# ç”¢ç”Ÿæ‰€æœ‰åœ–ç‰‡çš„ embedding\n",
        "\n",
        "combined_embeddings = []\n",
        "image_embeddings = []\n",
        "text_embeddings = []\n",
        "paths = []\n",
        "descriptions_list = []\n",
        "class_names_list = []\n",
        "\n",
        "# æ¬Šé‡è¨­å®š\n",
        "ALPHA = 0.6  # image æ¬Šé‡ï¼Œtext æ¬Šé‡ç‚º 1-ALPHA\n",
        "\n",
        "print(f\"é–‹å§‹ç”¢ç”Ÿ embedding (Î±={ALPHA})...\")\n",
        "print(f\"Combined = {ALPHA} Ã— Image + {1-ALPHA} Ã— Text\\n\")\n",
        "\n",
        "# éæ­·æ‰€æœ‰åœ–ç‰‡\n",
        "all_dirs = [d for d in os.listdir(IMAGE_FOLDER) if os.path.isdir(os.path.join(IMAGE_FOLDER, d))]\n",
        "\n",
        "for class_dir in tqdm(all_dirs, desc=\"Processing classes\"):\n",
        "    class_path = os.path.join(IMAGE_FOLDER, class_dir)\n",
        "    class_name = class_dir.split(\".\")[-1].replace(\"_\", \" \")\n",
        "\n",
        "    for f in os.listdir(class_path):\n",
        "        if not f.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
        "            continue\n",
        "\n",
        "        img_path = os.path.join(class_path, f)\n",
        "        rel_path = os.path.join(class_dir, f)\n",
        "\n",
        "        try:\n",
        "            # 1. Image Embedding\n",
        "            image = preprocess(Image.open(img_path)).unsqueeze(0).to(device)\n",
        "            with torch.no_grad():\n",
        "                img_emb = model.encode_image(image)\n",
        "                img_emb = img_emb / img_emb.norm(dim=-1, keepdim=True)\n",
        "\n",
        "            # 2. Text Embedding (å¾ text_c10 æè¿°)\n",
        "            if rel_path in descriptions and descriptions[rel_path]:\n",
        "                # å°æ‰€æœ‰æè¿°å–å¹³å‡ embedding\n",
        "                all_descs = descriptions[rel_path]\n",
        "                text_tokens = tokenizer(all_descs).to(device)\n",
        "                with torch.no_grad():\n",
        "                    text_emb = model.encode_text(text_tokens)\n",
        "                    text_emb = text_emb / text_emb.norm(dim=-1, keepdim=True)\n",
        "                    text_emb = text_emb.mean(dim=0, keepdim=True)  # å¹³å‡æ‰€æœ‰æè¿°\n",
        "                    text_emb = text_emb / text_emb.norm(dim=-1, keepdim=True)\n",
        "                desc_text = all_descs[0]  # ä¿å­˜ç¬¬ä¸€æ¢æè¿°\n",
        "            else:\n",
        "                # Fallback: ä½¿ç”¨é¡åˆ¥åç¨±\n",
        "                fallback_text = f\"a photo of a {class_name}\"\n",
        "                text_tokens = tokenizer([fallback_text]).to(device)\n",
        "                with torch.no_grad():\n",
        "                    text_emb = model.encode_text(text_tokens)\n",
        "                    text_emb = text_emb / text_emb.norm(dim=-1, keepdim=True)\n",
        "                desc_text = fallback_text\n",
        "\n",
        "            # 3. Combined Embedding\n",
        "            combined = ALPHA * img_emb + (1 - ALPHA) * text_emb\n",
        "            combined = combined / combined.norm(dim=-1, keepdim=True)\n",
        "\n",
        "            # å„²å­˜çµæœ\n",
        "            combined_embeddings.append(combined.cpu().numpy())\n",
        "            image_embeddings.append(img_emb.cpu().numpy())\n",
        "            text_embeddings.append(text_emb.cpu().numpy())\n",
        "            paths.append(img_path)\n",
        "            descriptions_list.append(desc_text)\n",
        "            class_names_list.append(class_name)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {img_path}: {e}\")\n",
        "            continue\n",
        "\n",
        "print(f\"\\nâœ… å®Œæˆï¼å…±è™•ç† {len(paths)} å¼µåœ–ç‰‡\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bc1e3f7",
      "metadata": {
        "id": "2bc1e3f7"
      },
      "source": [
        "## 3. å»ºç«‹ FAISS Index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c723679",
      "metadata": {
        "id": "6c723679"
      },
      "outputs": [],
      "source": [
        "import faiss\n",
        "\n",
        "# è½‰æ›ç‚º numpy array\n",
        "combined_array = np.vstack(combined_embeddings).astype(\"float32\")\n",
        "image_array = np.vstack(image_embeddings).astype(\"float32\")\n",
        "text_array = np.vstack(text_embeddings).astype(\"float32\")\n",
        "\n",
        "# å»ºç«‹ä¸‰ç¨® Index\n",
        "# 1. Combined Index (image + text)\n",
        "index_combined = faiss.IndexFlatIP(combined_array.shape[1])\n",
        "index_combined.add(combined_array)\n",
        "\n",
        "# 2. Image-only Index\n",
        "index_image = faiss.IndexFlatIP(image_array.shape[1])\n",
        "index_image.add(image_array)\n",
        "\n",
        "# 3. Text-only Index\n",
        "index_text = faiss.IndexFlatIP(text_array.shape[1])\n",
        "index_text.add(text_array)\n",
        "\n",
        "print(\"âœ… FAISS Index å»ºç«‹å®Œæˆï¼\")\n",
        "print(f\"   Combined Index: {index_combined.ntotal} å‘é‡\")\n",
        "print(f\"   Image Index:    {index_image.ntotal} å‘é‡\")\n",
        "print(f\"   Text Index:     {index_text.ntotal} å‘é‡\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89778481",
      "metadata": {
        "id": "89778481"
      },
      "source": [
        "## 4. æœå°‹å‡½æ•¸"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0967e415",
      "metadata": {
        "id": "0967e415"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from IPython.display import display\n",
        "\n",
        "# è¼‰å…¥æˆ–å»ºç«‹ path.json\n",
        "def load_path_mapping(path_json_file):\n",
        "    \"\"\"è¼‰å…¥ path.json æª”æ¡ˆå»ºç«‹ç´¢å¼•åˆ°è·¯å¾‘çš„å°æ‡‰\"\"\"\n",
        "    with open(path_json_file, 'r', encoding='utf-8') as f:\n",
        "        path_data = json.load(f)\n",
        "    return path_data\n",
        "\n",
        "def create_path_json(path_json_file, paths_list):\n",
        "    \"\"\"æ ¹æ“šç›®å‰çš„ paths åˆ—è¡¨å»ºç«‹ path.json\"\"\"\n",
        "    with open(path_json_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(paths_list, f, ensure_ascii=False, indent=2)\n",
        "    print(f\"âœ… å·²å»ºç«‹ {path_json_file}ï¼Œå…± {len(paths_list)} ç­†è·¯å¾‘\")\n",
        "\n",
        "# è¨­å®š path.json è·¯å¾‘\n",
        "PATH_JSON_FILE = os.path.join(BASE_PATH, \"path.json\")\n",
        "\n",
        "# è¼‰å…¥æˆ–å»ºç«‹è·¯å¾‘å°æ‡‰\n",
        "if os.path.exists(PATH_JSON_FILE):\n",
        "    path_mapping = load_path_mapping(PATH_JSON_FILE)\n",
        "    print(f\"âœ… å·²è¼‰å…¥ path.jsonï¼Œå…± {len(path_mapping)} ç­†è·¯å¾‘å°æ‡‰\")\n",
        "else:\n",
        "    # å¦‚æœä¸å­˜åœ¨ï¼Œå‰‡æ ¹æ“šç›®å‰çš„ paths åˆ—è¡¨å»ºç«‹\n",
        "    print(f\"âš ï¸ æ‰¾ä¸åˆ° {PATH_JSON_FILE}ï¼Œæ­£åœ¨å»ºç«‹...\")\n",
        "    create_path_json(PATH_JSON_FILE, paths)\n",
        "    path_mapping = paths  # ç›´æ¥ä½¿ç”¨ paths åˆ—è¡¨\n",
        "\n",
        "def get_image_path(index):\n",
        "    \"\"\"æ ¹æ“šç´¢å¼•å–å¾—åœ–ç‰‡è·¯å¾‘ï¼Œå„ªå…ˆä½¿ç”¨ path.json\"\"\"\n",
        "    if path_mapping is not None:\n",
        "        # æ ¹æ“š path.json çš„çµæ§‹å–å¾—è·¯å¾‘\n",
        "        # å¦‚æœæ˜¯ list æ ¼å¼\n",
        "        if isinstance(path_mapping, list):\n",
        "            return path_mapping[index]\n",
        "        # å¦‚æœæ˜¯ dict æ ¼å¼ (key æ˜¯ç´¢å¼•çš„å­—ä¸²)\n",
        "        elif isinstance(path_mapping, dict):\n",
        "            return path_mapping.get(str(index), paths[index])\n",
        "    # fallback åˆ°åŸæœ¬çš„ paths åˆ—è¡¨\n",
        "    return paths[index]\n",
        "\n",
        "def search_by_text(query, k=5, index_type=\"combined\"):\n",
        "    \"\"\"\n",
        "    ç”¨æ–‡å­—æœå°‹åœ–ç‰‡\n",
        "\n",
        "    Args:\n",
        "        query: æœå°‹æ–‡å­—\n",
        "        k: è¿”å›çµæœæ•¸é‡\n",
        "        index_type: \"combined\", \"image\", \"text\"\n",
        "    \"\"\"\n",
        "    text = tokenizer([query]).to(device)\n",
        "    with torch.no_grad():\n",
        "        query_emb = model.encode_text(text)\n",
        "        query_emb = query_emb / query_emb.norm(dim=-1, keepdim=True)\n",
        "        query_emb = query_emb.cpu().numpy().astype(\"float32\")\n",
        "\n",
        "    # é¸æ“‡ index\n",
        "    if index_type == \"combined\":\n",
        "        index = index_combined\n",
        "    elif index_type == \"image\":\n",
        "        index = index_image\n",
        "    else:\n",
        "        index = index_text\n",
        "\n",
        "    distances, indices = index.search(query_emb, k)\n",
        "\n",
        "    results = []\n",
        "    for j, i in enumerate(indices[0]):\n",
        "        results.append({\n",
        "            \"path\": get_image_path(i),\n",
        "            \"description\": descriptions_list[i],\n",
        "            \"class_name\": class_names_list[i],\n",
        "            \"score\": distances[0][j]\n",
        "        })\n",
        "    return results\n",
        "\n",
        "\n",
        "def search_by_image(img_path, k=5, index_type=\"combined\"):\n",
        "    \"\"\"\n",
        "    ç”¨åœ–ç‰‡æœå°‹ç›¸ä¼¼åœ–ç‰‡\n",
        "\n",
        "    Args:\n",
        "        img_path: åœ–ç‰‡è·¯å¾‘\n",
        "        k: è¿”å›çµæœæ•¸é‡\n",
        "        index_type: \"combined\", \"image\", \"text\"\n",
        "    \"\"\"\n",
        "    image = preprocess(Image.open(img_path)).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        query_emb = model.encode_image(image)\n",
        "        query_emb = query_emb / query_emb.norm(dim=-1, keepdim=True)\n",
        "        query_emb = query_emb.cpu().numpy().astype(\"float32\")\n",
        "\n",
        "    # é¸æ“‡ index\n",
        "    if index_type == \"combined\":\n",
        "        index = index_combined\n",
        "    elif index_type == \"image\":\n",
        "        index = index_image\n",
        "    else:\n",
        "        index = index_text\n",
        "\n",
        "    distances, indices = index.search(query_emb, k)\n",
        "\n",
        "    results = []\n",
        "    for j, i in enumerate(indices[0]):\n",
        "        results.append({\n",
        "            \"path\": get_image_path(i),\n",
        "            \"description\": descriptions_list[i],\n",
        "            \"class_name\": class_names_list[i],\n",
        "            \"score\": distances[0][j]\n",
        "        })\n",
        "    return results\n",
        "\n",
        "\n",
        "def show_results(results, show_description=True):\n",
        "    \"\"\"é¡¯ç¤ºæœå°‹çµæœ\"\"\"\n",
        "    for i, r in enumerate(results):\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"[{i+1}] {r['class_name']} (Score: {r['score']:.4f})\")\n",
        "        if show_description:\n",
        "            print(f\"    ğŸ“ {r['description'][:100]}...\")\n",
        "        display(Image.open(r['path']))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbe83845",
      "metadata": {
        "id": "cbe83845"
      },
      "source": [
        "## 5. æ¸¬è©¦æœå°‹"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "060fcd30",
      "metadata": {
        "id": "060fcd30"
      },
      "outputs": [],
      "source": [
        "# æ–‡å­—æœå°‹æ¸¬è©¦ - ä½¿ç”¨ Combined Index\n",
        "print(\"ğŸ” æœå°‹: 'gray kingbird' (Combined Index)\")\n",
        "results = search_by_text(\"short tail crow\", k=1, index_type=\"combined\")\n",
        "show_results(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49092193",
      "metadata": {
        "id": "49092193"
      },
      "outputs": [],
      "source": [
        "# æ–‡å­—æœå°‹æ¸¬è©¦ - ä½¿ç”¨è©³ç´°æè¿°\n",
        "print(\"ğŸ” æœå°‹: 'a small bird with yellow belly and black wings' (Combined Index)\")\n",
        "results = search_by_text(\"a small bird with yellow belly and black wings\", k=1, index_type=\"combined\")\n",
        "show_results(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8c113f2",
      "metadata": {
        "id": "e8c113f2"
      },
      "outputs": [],
      "source": [
        "# æ¯”è¼ƒä¸åŒ Index çš„æœå°‹çµæœ\n",
        "query = \"red bird with black head\"\n",
        "\n",
        "print(f\"ğŸ” æœå°‹: '{query}'\\n\")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"ğŸ“Š Combined Index çµæœ:\")\n",
        "results_combined = search_by_text(query, k=3, index_type=\"combined\")\n",
        "for r in results_combined:\n",
        "    print(f\"  - {r['class_name']} (score: {r['score']:.4f})\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ğŸ–¼ï¸ Image-only Index çµæœ:\")\n",
        "results_image = search_by_text(query, k=3, index_type=\"image\")\n",
        "for r in results_image:\n",
        "    print(f\"  - {r['class_name']} (score: {r['score']:.4f})\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ğŸ“ Text-only Index çµæœ:\")\n",
        "results_text = search_by_text(query, k=3, index_type=\"text\")\n",
        "for r in results_text:\n",
        "    print(f\"  - {r['class_name']} (score: {r['score']:.4f})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4119483a",
      "metadata": {
        "id": "4119483a"
      },
      "outputs": [],
      "source": [
        "# åœ–ç‰‡æœå°‹æ¸¬è©¦\n",
        "sample_img = paths[100]  # ä»»é¸ä¸€å¼µ\n",
        "print(f\"ğŸ–¼ï¸ æŸ¥è©¢åœ–ç‰‡: {class_names_list[100]}\")\n",
        "display(Image.open(sample_img))\n",
        "\n",
        "print(\"\\nğŸ” ç›¸ä¼¼åœ–ç‰‡ (Combined Index):\")\n",
        "results = search_by_image(sample_img, k=1, index_type=\"combined\")\n",
        "show_results(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecb7fffa",
      "metadata": {
        "id": "ecb7fffa"
      },
      "source": [
        "## 6. å„²å­˜ Index å’Œè³‡æ–™"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b10416f7",
      "metadata": {
        "id": "b10416f7"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# å„²å­˜ FAISS Index\n",
        "faiss.write_index(index_combined, \"cub200_combined.index\")\n",
        "faiss.write_index(index_image, \"cub200_image.index\")\n",
        "faiss.write_index(index_text, \"cub200_text.index\")\n",
        "\n",
        "# å„²å­˜ metadata\n",
        "metadata = {\n",
        "    \"paths\": paths,\n",
        "    \"descriptions\": descriptions_list,\n",
        "    \"class_names\": class_names_list,\n",
        "    \"alpha\": ALPHA\n",
        "}\n",
        "\n",
        "with open(\"cub200_metadata.pkl\", \"wb\") as f:\n",
        "    pickle.dump(metadata, f)\n",
        "\n",
        "print(\"âœ… å·²å„²å­˜:\")\n",
        "print(\"   - cub200_combined.index\")\n",
        "print(\"   - cub200_image.index\")\n",
        "print(\"   - cub200_text.index\")\n",
        "print(\"   - cub200_metadata.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b1718a0",
      "metadata": {
        "id": "1b1718a0"
      },
      "outputs": [],
      "source": [
        "# è¼‰å…¥å·²å„²å­˜çš„ Index (ä¹‹å¾Œä½¿ç”¨)\n",
        "import pickle\n",
        "import faiss\n",
        "#\n",
        "# # è¼‰å…¥ Index\n",
        "# index_combined = faiss.read_index(\"cub200_combined.index\")\n",
        "# index_image = faiss.read_index(\"cub200_image.index\")\n",
        "# index_text = faiss.read_index(\"cub200_text.index\")\n",
        "#\n",
        "# # è¼‰å…¥ metadata\n",
        "# with open(\"cub200_metadata.pkl\", \"rb\") as f:\n",
        "#     metadata = pickle.load(f)\n",
        "#     paths = metadata[\"paths\"]\n",
        "#     descriptions_list = metadata[\"descriptions\"]\n",
        "#     class_names_list = metadata[\"class_names\"]\n",
        "#\n",
        "# print(f\"è¼‰å…¥å®Œæˆï¼å…± {index_combined.ntotal} å‘é‡\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}